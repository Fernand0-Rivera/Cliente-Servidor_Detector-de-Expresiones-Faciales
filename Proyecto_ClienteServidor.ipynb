{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "!pip install opencv-python-headless"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import socket\n",
    "\n",
    "# Cargar el modelo \n",
    "#La ruta es dependiendo donde se encuentre el archivo h5 del modelo entrenado\n",
    "ruta_modelo = 'C:\\\\Users\\\\ferna\\\\OneDrive\\\\Documentos\\\\ElMejorModelo-2024-02-16.h5'\n",
    "best_model = load_model(ruta_modelo)\n",
    "\n",
    "def TakeSnapshot(filename='photo.jpg'):     # Función para capturar una foto\n",
    "    cap = cv2.VideoCapture(0)               # Abrir la cámara\n",
    "    ret, frame = cap.read()                 # Capturar un frame de la cámara\n",
    "    cv2.imwrite(filename, frame)            # Guardar el frame como una imagen en el archivo \n",
    "    cap.release()                           # Liberar los recursos de la cámara\n",
    "    return filename                         # Devolver el nombre del archivo en el que se guardó la imagen\n",
    "\n",
    "def detectar_expresion(s):                  # Función para detectar expresiones faciales\n",
    "    try:                                    # Toma una foto \n",
    "        filename = TakeSnapshot()           # Toma la foto y guarda en un archivo\n",
    "        print('Guardada en {}'.format(filename))    # Muestra el nombre donde fue guardada la imagen\n",
    "        image = cv2.imread(filename)        # Lee la imagen recién capturada\n",
    "        caras = face_cascade.detectMultiScale(image, 1.1, 4)    # Detecta caras en la imagen\n",
    "        if len(caras) > 0:                  # Si se detecta al menos una cara   \n",
    "            for (x, y, w, h) in caras:      # Por cada cara detectada, realiza la predicción de expresión\n",
    "                cara = image[y:y+h, x:x+w]\n",
    "                cara = cv2.resize(cara, (28, 28))\n",
    "                cara = cara / 255.0\n",
    "                cara = np.expand_dims(cara, axis=0)\n",
    "                \n",
    "                # Predice la expresión facial\n",
    "                prediccion = best_model.predict(cara)[0]\n",
    "                print(prediccion)\n",
    "                \n",
    "                # Obtiene la emoción predicha\n",
    "                indice_emocion_predicha = np.argmax(prediccion)\n",
    "                emocion_predicha = emociones[indice_emocion_predicha]\n",
    "                nombre_emocion_predicha = diccionario_emociones[emocion_predicha]['nombre']\n",
    "                valor_emocion_predicha = diccionario_emociones[emocion_predicha]['valor']\n",
    "\n",
    "                # Imprime la emoción detectada y muestra la imagen capturada con matplotlib\n",
    "                print(f\"Emoción inferida: {nombre_emocion_predicha}\")                \n",
    "                plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "                plt.title(f\"Emoción inferida: {nombre_emocion_predicha}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "                # Envía el valor de la emoción predicha a través del socket\n",
    "                comando = valor_emocion_predicha\n",
    "                s.send(comando.encode())\n",
    "                \n",
    "    except Exception as err:\n",
    "        print(str(err))\n",
    "\n",
    "# Configuración del socket\n",
    "s = socket.socket()\n",
    "s.connect((\"192.168.0.116\", 2020)) #La direccion IP es dependiendo la que muestre el servidor, y el puerto es propuesto por mi\n",
    "\n",
    "# Load the cascade\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# Emociones disponibles\n",
    "emociones = ['Angry', 'Happy', 'Neutral', 'Sad']\n",
    "# Diccionario de emociones\n",
    "diccionario_emociones = {\n",
    "    'Angry':    {'nombre': 'Enojo',     'valor': '1'},\n",
    "    'Happy':    {'nombre': 'Felicidad', 'valor': '2'},\n",
    "    'Neutral':  {'nombre': 'Neutral',   'valor': '3'},\n",
    "    'Sad':      {'nombre': 'Tristeza',  'valor': '4'}\n",
    "}\n",
    "\n",
    "# Tiempo de espera entre cada captura (en segundos)\n",
    "tiempo_espera = 10  # 10 segundos\n",
    "\n",
    "# Ejecuta un bucle continuo para tomar fotos cada 10 segundos\n",
    "while True:\n",
    "    detectar_expresion(s)  # Realiza la detección de expresiones faciales\n",
    "    time.sleep(tiempo_espera)  # Espera 10 segundos antes de la próxima captura\n"
   ],
   "id": "490d1668856b8a64"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
